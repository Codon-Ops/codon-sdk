{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Converting a LangGraph Workflow into a Codon Workload\n",
        "\n",
        "This notebook shows how to take an existing LangGraph graph built with LangChain components and wrap it with `CodonWorkload` using the `LangGraphWorkloadAdapter`. We get instrumentation, audit trails, logic IDs, and the token runtime without changing the LangGraph logic.\n",
        "\n",
        "> **Colab friendly** \u2013 install LangChain, LangGraph, and Codon SDK (from source) before running the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "\n",
        "Install dependencies and configure environment variables. Adjust paths as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --quiet langchain langgraph langchain-openai openai tiktoken\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "REPO_PATH = os.environ.get(\"CODON_SDK_PATH\", \"/content/codon-sdk\")\n",
        "if f\"{REPO_PATH}/sdk/src\" not in sys.path:\n",
        "    sys.path.append(f\"{REPO_PATH}/sdk/src\")\n",
        "\n",
        "# Replace with your own key or use Colab secrets\n",
        "os.environ.setdefault(\"OPENAI_API_KEY\", \"sk-REPLACE_ME\")\n",
        "os.environ.setdefault(\"ORG_NAMESPACE\", \"codon-langgraph-demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Build an Existing LangGraph Workflow\n",
        "\n",
        "We design a simple research workflow using LangChain tools:\n",
        "- Planner synthesises a plan.\n",
        "- Researcher expands insights.\n",
        "- Writer produces a summary.\n",
        "- Critic reviews and loop back if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "954abd10",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "llm_planner = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "llm_researcher = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "llm_writer = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "llm_critic = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "async def planner(state: Dict):\n",
        "    topic = state[\"topic\"]\n",
        "    prompt = f\"Draft a research plan for {topic}. Keep it short.\"\n",
        "    result = await llm_planner.ainvoke([HumanMessage(content=prompt)])\n",
        "    return {\"plan\": result.content}\n",
        "\n",
        "async def researcher(state: Dict):\n",
        "    prompt = (\n",
        "        \"Given this plan, provide three bullet insights.\n",
        "\"\n",
        "        f\"Plan:\n",
        "{state['plan']}\"\n",
        "    )\n",
        "    result = await llm_researcher.ainvoke([HumanMessage(content=prompt)])\n",
        "    return {\"insights\": result.content}\n",
        "\n",
        "async def writer(state: Dict):\n",
        "    prompt = (\n",
        "        \"Write a 120-word executive summary using the plan and insights.\n",
        "\"\n",
        "        f\"Plan: {state['plan']}\n",
        "\"\n",
        "        f\"Insights: {state['insights']}\"\n",
        "    )\n",
        "    result = await llm_writer.ainvoke([HumanMessage(content=prompt)])\n",
        "    return {\"draft\": result.content}\n",
        "\n",
        "async def critic(state: Dict):\n",
        "    prompt = (\n",
        "        \"Review this draft. Return JSON with fields 'decision' and 'feedback'.\n",
        "\"\n",
        "        \"Decision must be ACCEPT or REVISION.\"\n",
        "        f\"Draft: {state['draft']}\"\n",
        "    )\n",
        "    result = await llm_critic.ainvoke([HumanMessage(content=prompt)])\n",
        "    return {\"critique\": result.content}\n",
        "\n",
        "async def finalize(state: Dict):\n",
        "    return {\"summary\": state['draft'], \"critique\": state.get('critique', '')}\n",
        "\n",
        "class ResearchState(TypedDict, total=False):\n",
        "    topic: str\n",
        "    plan: str\n",
        "    insights: str\n",
        "    draft: str\n",
        "    critique: str\n",
        "\n",
        "graph = StateGraph(ResearchState)\n",
        "graph.add_node(\"planner\", planner)\n",
        "graph.add_node(\"researcher\", researcher)\n",
        "graph.add_node(\"writer\", writer)\n",
        "graph.add_node(\"critic\", critic)\n",
        "graph.add_node(\"finalize\", finalize)\n",
        "\n",
        "# Edges\n",
        "graph.add_edge(\"planner\", \"researcher\")\n",
        "graph.add_edge(\"researcher\", \"writer\")\n",
        "graph.add_edge(\"writer\", \"critic\")\n",
        "# cycle: critic -> writer if revision required\n",
        "graph.add_edge(\"critic\", \"writer\")\n",
        "# finalize when critic approves\n",
        "graph.add_edge(\"critic\", \"finalize\")\n",
        "\n",
        "graph.set_entry_point(\"planner\")\n",
        "graph.set_finish_point(\"finalize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run the LangGraph Workflow Directly\n",
        "\n",
        "Before wrapping with Codon, let's run the LangGraph graph to confirm it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_graph = graph.compile()\n",
        "initial_state = {\"topic\": \"Community gardens and urban wellbeing\"}\n",
        "result = await compiled_graph.ainvoke(initial_state)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convert to Codon Workload\n",
        "\n",
        "Using `LangGraphWorkloadAdapter` we can wrap the graph with zero manual instrumentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from codon.instrumentation.langgraph import (\n",
        "    initialize_telemetry,\n",
        "    LangGraphWorkloadAdapter,\n",
        ")\n",
        "\n",
        "initialize_telemetry(service_name=\"codon-langgraph-notebook\")\n",
        "\n",
        "codon_workload = LangGraphWorkloadAdapter.from_langgraph(\n",
        "    graph,\n",
        "    name=\"LangGraphResearchAgent\",\n",
        "    version=\"0.1.0\",\n",
        "    description=\"Wrapped LangGraph research agent\",\n",
        "    tags=[\"langgraph\", \"demo\"],\n",
        "    role_overrides={\n",
        "        \"planner\": \"planner\",\n",
        "        \"researcher\": \"analyst\",\n",
        "        \"writer\": \"author\",\n",
        "        \"critic\": \"qa\",\n",
        "        \"finalize\": \"publisher\",\n",
        "    },\n",
        ")\n",
        "codon_workload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Execute the Codon Workload\n",
        "\n",
        "Now we execute the workload with the same initial state. Tokens drive the LangGraph nodes under the hood, and we capture a ledger plus telemetry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = await codon_workload.execute_async({\"state\": initial_state}, deployment_id=\"notebook-demo\", max_steps=40)",
        "",
        "final_entry = report.node_results(\"finalize\")[-1]",
        "",
        "final_entry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Final summary:\n",
        "\", final_entry[\"summary\"][:400], \"\n",
        "...\")\n",
        "print(\"Critique:\n",
        "\", final_entry[\"critique\"])\n",
        "print(\"Iterations:\", report.context.get(\"iteration\"))\n",
        "print(\"Ledger entries:\", len(report.ledger))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspect Instruments\n",
        "\n",
        "```\n",
        "report.ledger[:5]\n",
        "```\n",
        "Use the ledger to review every node activation and token movement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for event in report.ledger[:10]:\n",
        "    print(event.event_type, event.source_node, \"->\", event.target_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Benefits Recap\n",
        "- **Telemetry**: `track_node` spans emitted automatically via the adapter.\n",
        "- **Audit trail**: `ExecutionReport.ledger` stores a full tape of the run.\n",
        "- **Logic IDs**: deterministic hashing of the LangGraph structure for idempotency.\n",
        "- **Developer ergonomics**: one call to `from_langgraph` replaces manual instrumentation.\n",
        "\n",
        "Next steps: explore the roadmap for persistence (`docs/vision/codon-workload-design-philosophy.md`) and the design guidelines for adapters (`docs/guides/workload-mixin-guidelines.md`)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "langgraph-to-codon-workload.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}